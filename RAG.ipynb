{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import pickle\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_split_text_from_pdf(pdf_path, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF and splits it into smaller chunks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = \" \".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "\n",
    "        # Use LangChain's RecursiveCharacterTextSplitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\"]\n",
    "        )\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting or splitting text from {pdf_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_and_split_texts_from_pdfs(pdf_folder, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    Load, extract, and split texts from all PDFs in a folder.\n",
    "    \"\"\"\n",
    "    pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "    texts, metadata = [], []\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        chunks = extract_and_split_text_from_pdf(pdf_file, chunk_size, chunk_overlap)\n",
    "        if chunks:\n",
    "            texts.extend(chunks)\n",
    "            metadata.extend([{\"filename\": os.path.basename(pdf_file), \"text\": chunk} for chunk in chunks])\n",
    "        else:\n",
    "            print(f\"Warning: No text extracted from {pdf_file}\")\n",
    "\n",
    "    return texts, metadata\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"\n",
    "    Create a FAISS index for given embeddings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings)\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating FAISS index: {e}\")\n",
    "        raise\n",
    "\n",
    "def save_faiss_index(index, metadata, index_path, metadata_path):\n",
    "    \"\"\"\n",
    "    Save FAISS index and metadata to disk.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(index_path), exist_ok=True)\n",
    "        faiss.write_index(index, index_path)\n",
    "        with open(metadata_path, \"wb\") as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        print(f\"FAISS index and metadata saved to {index_path} and {metadata_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving FAISS index or metadata: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_faiss_index(index_path, metadata_path):\n",
    "    \"\"\"\n",
    "    Load FAISS index and metadata from disk.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        index = faiss.read_index(index_path)\n",
    "        with open(metadata_path, \"rb\") as f:\n",
    "            metadata = pickle.load(f)\n",
    "        return index, metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FAISS index or metadata: {e}\")\n",
    "        raise\n",
    "\n",
    "def build_and_save_faiss_database(pdf_folder, output_folder, model_name=\"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Extract texts from PDFs, build FAISS index, and save both index and metadata.\n",
    "    \"\"\"\n",
    "    index_path = os.path.join(output_folder, \"index/faiss_index.bin\")\n",
    "    metadata_path = os.path.join(output_folder, \"index/metadata.pkl\")\n",
    "\n",
    "    texts, metadata = load_and_split_texts_from_pdfs(pdf_folder)\n",
    "    if not texts:\n",
    "        raise ValueError(\"No valid texts found in PDF folder!\")\n",
    "\n",
    "    # Initialize the embedding model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    index = create_faiss_index(embeddings)\n",
    "\n",
    "    # Save the index and metadata\n",
    "    save_faiss_index(index, metadata, index_path, metadata_path)\n",
    "\n",
    "def query_faiss_index(query, index, metadata, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Query FAISS index and return top results with metadata.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_embedding = model.encode([query])\n",
    "        distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            if idx < len(metadata):\n",
    "                results.append(metadata[idx])\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying FAISS index: {e}\")\n",
    "        return []\n",
    "\n",
    "def ollama_generate(query, context, api_url=\"http://localhost:11434/api/generate\", model=\"llama3.2\"):\n",
    "    \"\"\"\n",
    "    Generate an answer using the Ollama API with context, with error handling.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Use the following context to answer the question:\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Answer concisely:\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(api_url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"response\", \"No response from API.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error connecting to Ollama API: {e}\")\n",
    "        return \"Error: Unable to connect to API.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return \"Error: Unexpected issue during API call.\"\n",
    "\n",
    "def rag_with_faiss_database(query, output_folder, model_name=\"all-MiniLM-L6-v2\", api_url=\"http://localhost:11434/api/generate\"):\n",
    "    \"\"\"\n",
    "    Perform RAG using pre-built FAISS database.\n",
    "    \"\"\"\n",
    "    index_path = os.path.join(output_folder, \"index/faiss_index.bin\")\n",
    "    metadata_path = os.path.join(output_folder, \"index/metadata.pkl\")\n",
    "\n",
    "    # Load FAISS database\n",
    "    index, metadata = load_faiss_index(index_path, metadata_path)\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Query the database\n",
    "    results = query_faiss_index(query, index, metadata, model)\n",
    "\n",
    "    if not results:\n",
    "        return \"No relevant results found in the database.\"\n",
    "\n",
    "    # Compile context\n",
    "    context = \"\\n\\n\".join([res[\"text\"] for res in results])\n",
    "\n",
    "    # Generate response using Ollama\n",
    "    answer = ollama_generate(query, context, api_url)\n",
    "    return answer, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 58/58 [00:09<00:00,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index and metadata saved to RAG/FAISS/index/faiss_index.bin and RAG/FAISS/index/metadata.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_folder = \"RAG/papers\"\n",
    "output_folder = \"RAG/FAISS\"\n",
    "\n",
    "build_and_save_faiss_database(pdf_folder, output_folder, model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: The main concept of metacognition is the ability to think about, understand, and manage one's own cognitive processes, including knowledge about learning, monitoring, and regulating one's own thoughts, actions, and cognitions.\n",
      "Context: control and evaluation of them.  \n",
      "For Aguirre (2016) , metacognition is the ability of thinking that allows knowing what is known, planni ng \n",
      "strategies to do the action of knowing, being aware of thoughts during the knowledge process, and reflecting \n",
      "and evaluating the moments and actio ns of the knowledge process. McCluskey , Treffinger,  Baker and \n",
      "Lamoureux  (2013) Says that metacognition is the awareness of the learning processes itself, the strengths and\n",
      "\n",
      "among other things, to the active monitoring and consequent regulation and orchestration of these processes in relation to \n",
      "the cognitive objects or data on which they bear, usually in se rvice of some concrete goal or objective.\"[10]. In essence, \n",
      "metacognition is  the knowledge and the active monitoring of one's own cognitive processes. Indeed, we engage in metacognitive activ ities everyday.\n",
      "\n",
      "planning, monitoring, and evaluation [18]. \n",
      " \n",
      "Metacognition refers to the ability to think about, understand and manage one’s learning [19]. Metacognition includes \n",
      "knowledge about learning and about oneself as a learner, and the skills of monitoring and regulating one's own cognitive \n",
      "processes. Schraw and Dennison (1994) defined regulation of cognition as comprising of the following five aspects\n",
      "\n",
      "4 Department of Medical Psychology, Amsterdam University Medical Center, location VUmc, \n",
      "Amsterdam, Netherlands 54 A. Terneusen et al.\n",
      "1 3\n",
      "Introduction\n",
      "Metacognition is the ability to be conscious of your own thoughts, actions, and cognitions \n",
      "(Flavell, 1979; Stuss, 1991). This monitoring is necessary to adapt and achieve successful \n",
      "goal-directed behavior. There are many ways to define metacognition. Most models are\n",
      "\n",
      "versus their performance on examinations, or investigating students’ decisions during their learning\n",
      "process.Summary. Metacognition refers to people’s knowledge about and regulation of their cognitive pro-\n",
      "cesses. These aspects of metacognition are important for supporting students’ success in academic and\n",
      "experiential settings. In particular, students who recognize successful learning strategies can accurately\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the main concept of metacognition?\"\n",
    "answer, context = rag_with_faiss_database(\n",
    "    query=query,\n",
    "    output_folder=output_folder,\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    api_url=\"http://localhost:11434/api/generate\"\n",
    ")\n",
    "\n",
    "print(\"Generated Answer:\", answer)\n",
    "print(\"Context:\", context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
