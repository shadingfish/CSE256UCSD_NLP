{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "from collections import Counter\n",
    "import logging\n",
    "import pandas as pd\n",
    "from bg_context import class_bg\n",
    "import importlib\n",
    "import ollama_class\n",
    "\n",
    "# Reload the module to apply updates\n",
    "importlib.reload(ollama_class)\n",
    "\n",
    "# Re-import the class if necessary\n",
    "from ollama_class import OllamaLLM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = OllamaLLM(api_url=\"http://localhost:11434/api/generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义文件路径\n",
    "data_files = {\n",
    "    \"planning\": \"output_classification_generated_planning.csv\",\n",
    "    \"monitor\": \"output_classification_enhance_monitoring.csv\",\n",
    "    \"evaluating\": \"output_classification_generated_evaluating.csv\",\n",
    "}\n",
    "\n",
    "# 定义测试数据数量\n",
    "NUM_TEST_SAMPLES = 80\n",
    "\n",
    "MODES = [\"basic\", \"1_ex\", \"5_ex\", \"10_ex\", \"chain_of_thoughts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建文件夹结构\n",
    "def create_experiment_folder(mode):\n",
    "    base_dir = \"classification\"\n",
    "    timestamp_dir = os.path.join(base_dir, mode, time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    os.makedirs(timestamp_dir, exist_ok=True)\n",
    "    return timestamp_dir\n",
    "\n",
    "def create_experiment_folder_custom(path, mode):\n",
    "    base_dir = path\n",
    "    timestamp_dir = os.path.join(base_dir, mode, time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    os.makedirs(timestamp_dir, exist_ok=True)\n",
    "    return timestamp_dir\n",
    "\n",
    "# 读取CSV文件的函数\n",
    "def read_csv_data(file_path, max_rows=20):\n",
    "    \"\"\"\n",
    "    Reads up to max_rows rows from a CSV file.\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        max_rows (int): Maximum number of rows to read.\n",
    "    Returns:\n",
    "        list: A list of dictionaries with \"text\" and \"tag\" keys.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, \"r\") as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader)  # Skip the header row\n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "                data.append({\"text\": row[0], \"tag\": row[1]})\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# 初始化混淆矩阵\n",
    "def initialize_confusion_matrix(tags):\n",
    "    size = len(tags)\n",
    "    return np.zeros((size, size), dtype=int)\n",
    "\n",
    "# 更新混淆矩阵\n",
    "def update_confusion_matrix(matrix, y_true, y_pred, tags):\n",
    "    tag_to_index = {tag: idx for idx, tag in enumerate(tags)}\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true in tags and pred in tags:\n",
    "            matrix[tag_to_index[true], tag_to_index[pred]] += 1\n",
    "    return matrix\n",
    "\n",
    "# 绘制混淆矩阵热力图\n",
    "def generate_multiclass_heatmap(matrix, tags, output_path, mode):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=tags, yticklabels=tags)\n",
    "    plt.xlabel(\"Predicted Class\")\n",
    "    plt.ylabel(\"True Class\")\n",
    "    plt.title(f\"Confusion Matrix Heatmap for Mode: {mode}\")\n",
    "    plt.savefig(os.path.join(output_path, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# 计算分类性能指标\n",
    "def calculate_class_metrics(matrix, tags):\n",
    "    metrics = {}\n",
    "    for idx, tag in enumerate(tags):\n",
    "        tp = matrix[idx, idx]\n",
    "        fp = matrix[:, idx].sum() - tp\n",
    "        fn = matrix[idx, :].sum() - tp\n",
    "        tn = matrix.sum() - (tp + fp + fn)\n",
    "\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1_score = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "        accuracy = (tp + tn) / matrix.sum() if matrix.sum() > 0 else 0\n",
    "\n",
    "        metrics[tag] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"accuracy\": accuracy\n",
    "        }\n",
    "    return metrics\n",
    "\n",
    "def safe_parse_json(response):\n",
    "    \"\"\"\n",
    "    Safely parse a JSON string into a Python dictionary.\n",
    "    Args:\n",
    "        response (str): JSON string to parse.\n",
    "    Returns:\n",
    "        dict: Parsed dictionary if successful.\n",
    "    Raises:\n",
    "        ValueError: If the input is not valid JSON.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed = json.loads(response)\n",
    "        if not isinstance(parsed, dict):\n",
    "            raise ValueError(\"Parsed JSON is not a dictionary.\")\n",
    "        return parsed\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Invalid JSON format: {response[:100]}\") from e\n",
    "\n",
    "def evaluate_classification(data, mode, model: OllamaLLM, all_tags):\n",
    "    results = []\n",
    "    y_true, y_pred = [], []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for item in data:\n",
    "        text = item[\"text\"]\n",
    "        true_tag = item[\"tag\"]\n",
    "        try:\n",
    "            # 调用分类函数\n",
    "            response = model.classify_metacognition(model=\"llama3.2\", text=text, context=class_bg, mode=mode)\n",
    "            \n",
    "            # 安全解析 JSON 响应\n",
    "            response_dict = safe_parse_json(response)\n",
    "            predicted_tag = response_dict.get(\"classification\", \"na\")\n",
    "\n",
    "            y_true.append(true_tag)\n",
    "            y_pred.append(predicted_tag)\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"true_tag\": true_tag,\n",
    "                \"predicted_tag\": predicted_tag,\n",
    "                \"error\": False\n",
    "            })\n",
    "        except ValueError as ve:\n",
    "            # JSON 解析失败\n",
    "            y_true.append(true_tag)\n",
    "            y_pred.append(\"error\")\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"true_tag\": true_tag,\n",
    "                \"predicted_tag\": \"error\",\n",
    "                \"error\": True,\n",
    "                \"error_message\": f\"JSON parsing error: {ve}\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            # 其他异常\n",
    "            y_true.append(true_tag)\n",
    "            y_pred.append(\"error\")\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"true_tag\": true_tag,\n",
    "                \"predicted_tag\": \"error\",\n",
    "                \"error\": True,\n",
    "                \"error_message\": f\"Unexpected error: {str(e)}\"\n",
    "            })\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    avg_time_per_sample = total_time / len(data) if data else 0\n",
    "\n",
    "    # 构建混淆矩阵\n",
    "    confusion_matrix = initialize_confusion_matrix(all_tags)\n",
    "    confusion_matrix = update_confusion_matrix(confusion_matrix, y_true, y_pred, all_tags)\n",
    "\n",
    "    # 计算分类性能\n",
    "    metrics = calculate_class_metrics(confusion_matrix, all_tags)\n",
    "    metrics[\"total_time\"] = total_time\n",
    "    metrics[\"avg_time_per_sample\"] = avg_time_per_sample\n",
    "\n",
    "    return metrics, results, confusion_matrix\n",
    "\n",
    "# 保存结果到文件\n",
    "def save_results(metrics, results, confusion_matrix, output_path, tags):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # 保存混淆矩阵热力图\n",
    "    generate_multiclass_heatmap(confusion_matrix, tags, output_path, mode=\"classification\")\n",
    "    \n",
    "    # 保存分类指标\n",
    "    metrics_path = os.path.join(output_path, \"metrics.json\")\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    # 保存详细结果\n",
    "    results_path = os.path.join(output_path, \"results.json\")\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mode: basic\n",
      "Processing dataset for tag: planning\n",
      "Results for tag planning saved in classification/basic/20241204_174518\n",
      "Processing dataset for tag: monitor\n",
      "Results for tag monitor saved in classification/basic/20241204_174518\n",
      "Processing dataset for tag: evaluating\n",
      "Results for tag evaluating saved in classification/basic/20241204_174518\n",
      "Overall results and metrics saved for mode: basic\n",
      "Processing mode: 1_ex\n",
      "Processing dataset for tag: planning\n",
      "Results for tag planning saved in classification/1_ex/20241204_174937\n",
      "Processing dataset for tag: monitor\n",
      "Results for tag monitor saved in classification/1_ex/20241204_174937\n",
      "Processing dataset for tag: evaluating\n",
      "Results for tag evaluating saved in classification/1_ex/20241204_174937\n",
      "Overall results and metrics saved for mode: 1_ex\n",
      "Processing mode: 5_ex\n",
      "Processing dataset for tag: planning\n",
      "Results for tag planning saved in classification/5_ex/20241204_175348\n",
      "Processing dataset for tag: monitor\n",
      "Results for tag monitor saved in classification/5_ex/20241204_175348\n",
      "Processing dataset for tag: evaluating\n",
      "Results for tag evaluating saved in classification/5_ex/20241204_175348\n",
      "Overall results and metrics saved for mode: 5_ex\n",
      "Processing mode: 10_ex\n",
      "Processing dataset for tag: planning\n",
      "Results for tag planning saved in classification/10_ex/20241204_175812\n",
      "Processing dataset for tag: monitor\n",
      "Results for tag monitor saved in classification/10_ex/20241204_175812\n",
      "Processing dataset for tag: evaluating\n",
      "Results for tag evaluating saved in classification/10_ex/20241204_175812\n",
      "Overall results and metrics saved for mode: 10_ex\n",
      "Processing mode: chain_of_thoughts\n",
      "Processing dataset for tag: planning\n",
      "Results for tag planning saved in classification/chain_of_thoughts/20241204_180248\n",
      "Processing dataset for tag: monitor\n",
      "Results for tag monitor saved in classification/chain_of_thoughts/20241204_180248\n",
      "Processing dataset for tag: evaluating\n",
      "Results for tag evaluating saved in classification/chain_of_thoughts/20241204_180248\n",
      "Overall results and metrics saved for mode: chain_of_thoughts\n",
      "All experiments completed.\n"
     ]
    }
   ],
   "source": [
    "# 主测试逻辑\n",
    "all_tags = list(data_files.keys()) + [\"na\"]  # 包括所有可能的标签\n",
    "\n",
    "for mode in MODES:\n",
    "    print(f\"Processing mode: {mode}\")\n",
    "    mode_output_path = create_experiment_folder(mode)\n",
    "\n",
    "    # 初始化模式级别混淆矩阵和结果列表\n",
    "    overall_confusion_matrix = initialize_confusion_matrix(all_tags)\n",
    "    overall_results = []\n",
    "\n",
    "    for tag, file_path in data_files.items():\n",
    "        try:\n",
    "            print(f\"Processing dataset for tag: {tag}\")\n",
    "            # 读取数据\n",
    "            data = read_csv_data(file_path, max_rows=NUM_TEST_SAMPLES)\n",
    "            if not data:\n",
    "                print(f\"Warning: No data found in {file_path}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            # 执行分类评估\n",
    "            metrics, results, confusion_matrix = evaluate_classification(data, mode, ollama_model, all_tags)\n",
    "\n",
    "            # 更新模式级别的混淆矩阵\n",
    "            overall_confusion_matrix = update_confusion_matrix(\n",
    "                overall_confusion_matrix,\n",
    "                [r[\"true_tag\"] for r in results],\n",
    "                [r[\"predicted_tag\"] for r in results],\n",
    "                all_tags\n",
    "            )\n",
    "            overall_results.extend(results)  # 合并所有文件的结果\n",
    "\n",
    "            # 保存单文件的结果\n",
    "            save_results(metrics, results, confusion_matrix, mode_output_path, all_tags)\n",
    "            print(f\"Results for tag {tag} saved in {mode_output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing dataset for tag {tag}: {e}\")\n",
    "\n",
    "    # 计算模式级别的总指标\n",
    "    try:\n",
    "        overall_metrics = calculate_class_metrics(overall_confusion_matrix, all_tags)\n",
    "\n",
    "        # 保存模式级别的结果\n",
    "        generate_multiclass_heatmap(overall_confusion_matrix, all_tags, mode_output_path, mode)\n",
    "        metrics_path = os.path.join(mode_output_path, \"overall_metrics.json\")\n",
    "        with open(metrics_path, \"w\") as f:\n",
    "            json.dump(overall_metrics, f, indent=4)\n",
    "\n",
    "        # 保存合并的详细结果\n",
    "        results_path = os.path.join(mode_output_path, \"overall_results.json\")\n",
    "        with open(results_path, \"w\") as f:\n",
    "            json.dump(overall_results, f, indent=4)\n",
    "\n",
    "        print(f\"Overall results and metrics saved for mode: {mode}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating overall results for mode {mode}: {e}\")\n",
    "\n",
    "print(\"All experiments completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_classification(data, mode, model: OllamaLLM, tags, classification_method=\"classify_metacognition\"):\n",
    "#     results = []\n",
    "#     y_true, y_pred = [], []\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     for item in data:\n",
    "#         text = item[\"text\"]\n",
    "#         true_tag = item[\"tag\"]\n",
    "#         try:\n",
    "#             # 动态调用分类方法\n",
    "#             response = getattr(model, classification_method)(model=\"llama3.2\", text=text, context=class_bg, mode=mode)\n",
    "#             predicted_tag = response.get(\"classification\", \"na\")\n",
    "\n",
    "#             y_true.append(true_tag)\n",
    "#             y_pred.append(predicted_tag)\n",
    "#             results.append({\n",
    "#                 \"text\": text,\n",
    "#                 \"true_tag\": true_tag,\n",
    "#                 \"predicted_tag\": predicted_tag,\n",
    "#                 \"error\": False\n",
    "#             })\n",
    "#         except Exception as e:\n",
    "#             y_true.append(true_tag)\n",
    "#             y_pred.append(\"error\")\n",
    "#             results.append({\n",
    "#                 \"text\": text,\n",
    "#                 \"true_tag\": true_tag,\n",
    "#                 \"predicted_tag\": \"error\",\n",
    "#                 \"error\": True,\n",
    "#                 \"error_message\": str(e)\n",
    "#             })\n",
    "\n",
    "#     total_time = time.time() - start_time\n",
    "#     avg_time_per_sample = total_time / len(data) if data else 0\n",
    "\n",
    "#     confusion_matrix = initialize_confusion_matrix(tags)\n",
    "#     confusion_matrix = update_confusion_matrix(confusion_matrix, y_true, y_pred, tags)\n",
    "\n",
    "#     metrics = calculate_class_metrics(confusion_matrix, tags)\n",
    "#     metrics[\"total_time\"] = total_time\n",
    "#     metrics[\"avg_time_per_sample\"] = avg_time_per_sample\n",
    "\n",
    "#     return metrics, results, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"monitor\": \"output_classification_enhance_monitoring.csv\",\n",
    "    \"na\": \"output_classification_generated_na.csv\",\n",
    "}\n",
    "NUM_TEST_SAMPLES = 100\n",
    "MODES = [\"basic\", \"1_ex\", \"5_ex\", \"10_ex\", \"chain_of_thoughts\"]\n",
    "tags = [\"monitor\", \"na\"]\n",
    "\n",
    "class_bg_na_or_monitor = \"\"\"\n",
    "The input text is a student's reflection after completing a computer science test on the CompassX platform, \n",
    "an online CS learning tool. Students are asked to analyze their performance, \n",
    "focusing on what they did well, where they struggled, and how they plan to address any mistakes. \n",
    "Responses are often informal, fragmented, and focused on specific programming concepts or errors. \n",
    "Despite this, the reflections frequently indicate monitoring behaviors such as identifying mistakes or tracking progress.\n",
    "\"\"\"\n",
    "\n",
    "base_output_path = \"classification_monitor_na\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mode: basic\n",
      "Processing dataset for tag: monitor\n",
      "Processing dataset for tag: na\n",
      "Overall results and metrics saved for mode: basic\n",
      "Processing mode: 1_ex\n",
      "Processing dataset for tag: monitor\n",
      "Processing dataset for tag: na\n",
      "Overall results and metrics saved for mode: 1_ex\n",
      "Processing mode: 5_ex\n",
      "Processing dataset for tag: monitor\n",
      "Processing dataset for tag: na\n",
      "Overall results and metrics saved for mode: 5_ex\n",
      "Processing mode: 10_ex\n",
      "Processing dataset for tag: monitor\n",
      "Processing dataset for tag: na\n",
      "Overall results and metrics saved for mode: 10_ex\n",
      "Processing mode: chain_of_thoughts\n",
      "Processing dataset for tag: monitor\n",
      "Processing dataset for tag: na\n",
      "Overall results and metrics saved for mode: chain_of_thoughts\n",
      "Monitor/NA classification pipeline completed.\n"
     ]
    }
   ],
   "source": [
    "for mode in MODES:\n",
    "    print(f\"Processing mode: {mode}\")\n",
    "    mode_output_path = create_experiment_folder_custom(base_output_path, mode)\n",
    "\n",
    "    # 初始化模式级别混淆矩阵和结果列表\n",
    "    overall_confusion_matrix = initialize_confusion_matrix(tags)\n",
    "    overall_results = []\n",
    "\n",
    "    for tag, file_path in data_files.items():\n",
    "        try:\n",
    "            print(f\"Processing dataset for tag: {tag}\")\n",
    "            data = read_csv_data(file_path, max_rows=NUM_TEST_SAMPLES)\n",
    "            if not data:\n",
    "                print(f\"Warning: No data found in {file_path}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            y_true, y_pred = [], []\n",
    "            results = []\n",
    "            start_time = time.time()\n",
    "\n",
    "            for item in data:\n",
    "                text = item[\"text\"]\n",
    "                true_tag = item[\"tag\"]\n",
    "                try:\n",
    "                    # 调用 classify_monitor_or_na 方法\n",
    "                    response = ollama_model.classify_monitor_or_na(\n",
    "                        model=\"llama3.2\", text=text, context=class_bg_na_or_monitor, mode=mode\n",
    "                    )\n",
    "                    response = json.loads(response)\n",
    "                    predicted_tag = response.get(\"classification\", \"na\")\n",
    "                    y_true.append(true_tag)\n",
    "                    y_pred.append(predicted_tag)\n",
    "                    results.append({\n",
    "                        \"text\": text,\n",
    "                        \"true_tag\": true_tag,\n",
    "                        \"predicted_tag\": predicted_tag,\n",
    "                        \"error\": False\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    y_true.append(true_tag)\n",
    "                    y_pred.append(\"error\")\n",
    "                    results.append({\n",
    "                        \"text\": text,\n",
    "                        \"true_tag\": true_tag,\n",
    "                        \"predicted_tag\": \"error\",\n",
    "                        \"error\": True,\n",
    "                        \"error_message\": str(e)\n",
    "                    })\n",
    "\n",
    "            total_time = time.time() - start_time\n",
    "            avg_time_per_sample = total_time / len(data) if data else 0\n",
    "\n",
    "            # 更新混淆矩阵\n",
    "            confusion_matrix = initialize_confusion_matrix(tags)\n",
    "            confusion_matrix = update_confusion_matrix(confusion_matrix, y_true, y_pred, tags)\n",
    "            overall_confusion_matrix = update_confusion_matrix(overall_confusion_matrix, y_true, y_pred, tags)\n",
    "\n",
    "            # 保存单文件结果\n",
    "            metrics = calculate_class_metrics(confusion_matrix, tags)\n",
    "            metrics[\"total_time\"] = total_time\n",
    "            metrics[\"avg_time_per_sample\"] = avg_time_per_sample\n",
    "\n",
    "            # 保存结果\n",
    "            with open(os.path.join(mode_output_path, f\"{tag}_results.json\"), \"w\") as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "            with open(os.path.join(mode_output_path, f\"{tag}_metrics.json\"), \"w\") as f:\n",
    "                json.dump(metrics, f, indent=4)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing dataset for tag {tag}: {e}\")\n",
    "\n",
    "    # 保存模式级别结果\n",
    "    try:\n",
    "        overall_metrics = calculate_class_metrics(overall_confusion_matrix, tags)\n",
    "        generate_multiclass_heatmap(overall_confusion_matrix, tags, mode_output_path, mode)\n",
    "\n",
    "        # 保存总体结果\n",
    "        with open(os.path.join(mode_output_path, \"overall_results.json\"), \"w\") as f:\n",
    "            json.dump(overall_results, f, indent=4)\n",
    "        with open(os.path.join(mode_output_path, \"overall_metrics.json\"), \"w\") as f:\n",
    "            json.dump(overall_metrics, f, indent=4)\n",
    "\n",
    "        print(f\"Overall results and metrics saved for mode: {mode}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating overall results for mode {mode}: {e}\")\n",
    "\n",
    "print(\"Monitor/NA classification pipeline completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
